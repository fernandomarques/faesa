<!doctype html>
<html lang="pt">

	<head>
		<meta charset="utf-8">

		<title>Estatística para Data Science II</title>

		<meta name="author" content="Fernando Antonio Marques Filho">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="../css/reveal.css">
		<link rel="stylesheet" href="../css/misc.css">
		<link rel="stylesheet" href="../css/theme/sky.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="../lib/css/zenburn.css">
		<style>
			img {
				border : 0px !important;
			}
		</style>
		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? '../css/print/pdf.css' : '../css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>
	<body>
		<div class="reveal">
			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h3>Aprendizagem Não Supervisionada</h3>					<p>
						<small>Created by Fernando Marques </small>
					</p>
		</section>
		<section>
			<img src="img/machinelearning.jpg" alt="Infografico machine learning" >
		</section>

		<section data-markdown="">
			> O termo _aprendizagem não supervisionada_ se aplica a métodos estatísticos que extraem significado de dados
			sem trainar o modelo em dados não rotulados.
			> Na aprendizagem não supervisionado também teremos um modelo dos dados, mas não há distinção entre variáveis
			preditoras e variáveis de resposta

		</section>
		<section data-markdown>
			## Aprendizagem não supervisionada
			- Podemos ter objetivos diferentes como identificar grupos significativos
			- Pode ser usado para definir perfis de usuários
			- Também pode ser utilizado para reduzir a dimensão dos dados
			- Ou até mesmo como uma extensão da análise exploratória dos dados
		</section>
		<section data-markdown>
			## Aprendizagem não supervisionada
			- Pode ser importante para classificações e regressões, quando queremos uma categoria de dados não rotulados
				- Identificar tipo de vegetação com base em dados de satélite, como não temos a resposta, clustering identifica
				regiões com padrões similares
			- Clustering também é uma ferramenta para problemas de partida a frio, quando ainda não temos uma resposta para 
			treinar o modelo, vamos aprendendo sobre os dados com a evolução do negócio
		
		</section>
		<section data-markdown>
			## Aprendizagem não supervisionada
		
			- Também pode ser útil para os casos de classe rara como em subpopulações, o clustering pode criar features
			que representem a subpopulação. Pode ser criado modelos para cada tipo de população, ou forçar que o modelo
			use a subpopulação como preditor
		</section>
		<section data-markdown>
			## Análise de Componentes Principais
			- Muitas vezes variáveis vão variar juntas, o PCA é uma técnica para descobrir como variáveis numéricas 
			covariam
			- A ideia é combinar múltiplos preditores numéricos em um conjunto de variáveis
			- Os pesos usados para formar os componentes principais revelam as contribuições das variáveis originais
		</section>
		<section data-markdown>
			## PCA
			- Para duas variáveis $X_1$ e $X_2$ existem dois componentes principais $Z_i(i=1\,ou\,2)$
			$$ Z_i = w_i1 X_1 + w_i2X_2 $$
			- Os pesos w(i,1) e w(i,2) são conhecidos como carregamentos do componente e transformam as variáveis originais
			no PCA
			- O primeiro componente $Z_1$ é a combinação linear que melhor explica a variação total
			- $Z_2$ explica o restante da variação
		</section>
		<section>
			<pre><code>
oil_px &lt;- sp500_px[, c('CVX', 'XOM')]
oil_px = as.data.frame(scale(oil_px, scale=FALSE))
pca &lt;- princomp(oil_px)
pca$loadings
			</code></pre>
			<p>O primeiro componente é uma média dos dois, refletindo a correlação entre as empresas de energia</p>
			<p>O segundo componente mede quando os valores entre as duas divergem</p>
		</section>
		<section>
			<pre><code>
loadings &lt; pca$loadings
ggplot(data=oil_px, aes(x=CVX, y=XOM)) +
	geom_point(alpha=.3) +
	scale_shape_manual(values=c(46)) +
	stat_ellipse(type='norm', level=.99, color='grey25') +
	geom_abline(intercept = 0, slope = loadings[2,1]/loadings[1,1], color='grey25', linetype=2) +
	geom_abline(intercept = 0, slope = loadings[2,2]/loadings[1,2],  color='grey25', linetype=2) +
	scale_x_continuous(expand=c(0,0), lim=c(-3, 3)) + 
	scale_y_continuous(expand=c(0,0), lim=c(-3, 3)) +
	theme_bw()
			</code></pre>
			<p>As linhas mostram os dois componentes principais. A primeira é a linha no maior eixo da elipse e mostra a maior parte da variabilidade</p>
		</section>
		<section data-markdown>
			## Calculando PC
			- Para ir de duas variáveis para mais basta incluir o preditor adicional na combinação linear e colocar 
			pesos que optimizem a coleção de covariação de todas variáveis preditoras no primeiro componente principal
			- O cálculo do componente principal é um método estatístico clássico que usa a matriz de correlação ou 
			a matriz de covariância
			- Sua execução é extremamente rápida e não possui iterações
			- Só funciona com variáveis numéricas
		</section>
		<section data-markdown>
			## Calculando PC
			1. PCA encontra combinações lineares de preditores que maximize o percentual de variância explicado
			1. Essa combinação linear é o primeiro preditor $Z_1$
			1. PCA repete o processo usando as mesmas variáveis e pesos diferentes é gerado o segundo preditor $Z_2$, os
			pesos são feitos de forma que $Z_1$ e $Z_2$ não sejam correlacionados
			1. O processo continua até que você tenha tantas variáveis, ou componentes, quanto o número original de variáveis
			1. Escolha quantos componentes necessários para considerar a maior parte da variância
		</section>
		<section data-markdown>
			## Calculando PC
			- O resultado é um conjunto de pesos para cada componente, o passo final é converter os dados originais em novos
			componentes principais aplicando os pesos aos valores
		</section>
		<section data-markdown>
			## Interpretando PC
			- A natureza dos componente principais revela informações a respeito da estrutura dos dados
			- Um método de ver os componentes é o _screeplot_ 
		</section>
		<section><pre><code>
syms &lt;- c( 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB', 'COP',
	'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')
 top_cons &lt;- sp500_px[row.names(sp500_px)>='2011-01-01', syms]
 sp_pca &lt;- princomp(top_cons)
 par(mar=c(6,3,0,0)+.1, las=2)
 screeplot(sp_pca, main='')
		</code></pre>
		<p>O eixo Y mostra a variância, dá para ver que o primeiro é bem grande</p>
	</section>
	<section>
		<pre><code>
library(tidyr)
loadings = sp_pca$loadings[,1:5]
loadings &lt;- as.data.frame(loadings)
loadings$Symbol &lt;- row.names(loadings)
loadings &lt;- gather(loadings, "Component", "Weight", -Symbol)
loadings$Color = loadings$Weight > 0
ggplot(loadings, aes(x=Symbol, y=Weight, fill=Color)) +
	geom_bar(stat='identity', position = "identity", width=.75) + 
	facet_grid(Component ~ ., scales='free_y') +
	guides(fill=FALSE)  +
	ylab('Component Loading') +
	theme_bw() +
	theme(axis.title.x = element_blank(),
		axis.text.x  = element_text(angle=90, vjust=0.5))
		</code></pre>
		<p>Comentários no proximo slide</p>
	</section>
	<section data-markdown>
		## Interpretando PC
		- No primeiro componente todos possuem o mesmo sentido, é comum para dados em que todas colunas tenham um fator em comum
		- O segundo componente compara a as mudanças de preço de ações de energia comparadas a outras ações
		- O terceiro mostra o contraste entre Apple e CostCo.
		- O quarto mostra ações da Schlumberger a outras ações de energia
		- A quinta é dominada por empresas de finanças
	</section>
	<section data-markdown>
		## Quantos componentes?
		- O mais comum é usar uma regra ad hoc para selecionar os componentes que mais explicam a variância
		- Também podemos escolher os primeiros até explicar 80% do total
		- Um método mais formal é utilizar validação cruzada!
	</section>
	<section data-markdown>
		- Componentes principais são combinações lineares das variáveis preditoras numéricas
		- É calculado minimizando a correlação entre os componentes
		- Um número limitado de componentes costuma explicar a maior parte dos dados
		- Um conjunto limitados dos componentes pode ser usado no lugar dos preditores originais, diminuindo a dimensionalidade
	</section>
	<section data-markdown>
		## K-Means Clustering
		- Clustering é uma técnica que divide os dados em grupos diferentes, onde os dados são similares entre os grupos
		- Um dos objetivos é identificar grupos significativos
		- Foi o primeiro método de clustering desenvolvido e ainda é muito utilizado
		- K-means significa dividir os dados em K clusters minimizando a soma do quadrado das distâncias dos registros ao 
		centro do cluster (mean)
		- Não garante que os clusters terão mesmo tamanho mas encontra clusters que se separam melhor
		- NORMALIZAR!!!
	</section>
	<section data-markdown>
		## K-means
		- Comece considerando uma base com n registros e apenas duas variáveis
		- Queremos dividir os dados em K=4, cada registro será ligado a um dos 4 clusters
		- O centro de um cluster é a média dos pontos do cluster
		$$ \bar{x} = \frac{1}{n_k} \sum x_i, i \in cluster \, k $$
		- O mesmo para $\bar{y} $
		- A soma dos quadrados do cluster é
		$$ SS_k = \sum_i (x_i - \bar{x}_k)^2 + (y_i + \bar{y}_k)^2, i  \in Cluster\,K$$
	</section>
	<section data-markdown>
		## K-means
		- O algoritmo minimiza a soma dos quadrados dos clusters para todos os clusters
		$$ \sum_{k=1}^4 SS_k $$
		- Pode ser usado para gerar insight de como a movimentação nos preços de ações se agrupam
		- Os retornos das ações já são dados de forma que não seja necessário normalizar
	</section>
	<section>
		<pre><code>
set.seed(1010103)
df &lt;- sp500_px[row.names(sp500_px)>='2011-01-01', c('XOM', 'CVX')]
km &lt;- kmeans(df, centers=4, nstart=1)

df$cluster &lt;- factor(km$cluster)
head(df)

centers &lt;- data.frame(cluster=factor(1:4), km$centers)
centers
		</code></pre>
		<p>Alguns  representam quedas nas bolsas e os outros clusters subidas</p>
	</section>
	<section>
		<pre><code>
ggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +
geom_point(alpha=.3) +
scale_shape_manual(values = 1:4,
	guide = guide_legend(override.aes=aes(size=1))) + 
geom_point(data=centers,  aes(x=XOM, y=CVX), size=2, stroke=2)  +
theme_bw() +
scale_x_continuous(expand=c(0,0), lim=c(-2, 2)) + 
scale_y_continuous(expand=c(0,0), lim=c(-2.5, 2.5)) 
		</code></pre>
	</section>
	<section data-markdown>
		## Kmeans Algoritmo
		- Pode ser aplicado a dados com P preditores, apesar da solução exata ser difícil, heurísticas são eficientes
		- O algoritmo começa com um K e um conjunto de média de clusters
			1. Adicione cada registro ao cluster mais próximo
			1. Calcule o a nova média do cluster
			1. Repita
		- Como não há garantia de que o algoritmo encontre a melhor solução, é recomendado executar algumas vezes com reamostras
		- O parâmetro `nstart` indica quantos inícios diferentes e retorna o melhor
	</section>
	<section>
		<pre><code>
syms &lt;- c( 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB', 'COP',
'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')
df &lt;- sp500_px[row.names(sp500_px)>='2011-01-01', syms]

set.seed(10010)
km &lt;- kmeans(df, centers=5, nstart=10)
km$size
centers &lt;- km$centers
		</code></pre>
		<p>Temos cluster para alta e baixa no mercado, alto no mercado de consumo e baixa de energia e
			consumo em alta e energia em baixa
		</p>
	</section>
	<section data-markdown>
		## Escolhendo K
		- Não existe um método padrão para melhor número de K
		- Se queremos dividir os consumidores, K=2 pode ser pouco
		- Uma forma usada para definir K é o _método do cotovelo_
		- Não funciona para todos os dados
	</section>
	<section>
		<pre><code>
pct_var &lt;- data.frame(pct_var = 0, num_clusters=2:14)
totalss &lt;- kmeans(df, centers=14, nstart=50, iter.max = 100)$totss
for(i in 2:14){
pct_var[i-1, 'pct_var'] &lt;- kmeans(df, centers=i, nstart=50, iter.max = 100)$betweenss/totalss
}

png(filename=file.path(PSDS_PATH, 'figures', 'psds_0706.png'), width = 4, height=3, units='in', res=300)

ggplot(pct_var, aes(x=num_clusters, y=pct_var)) +
geom_line() +
geom_point() +
labs(y='% Variance Explained', x='Number of Clusters') +
scale_x_continuous(breaks=seq(2, 14, by=2))   +
theme_bw()
		</code></pre>
		<p>Esse comportamento é comum em dados que não tem clusters bem definidos</p>
	</section>
	<section>
		<img src="img/Modulo6Elbow.png" alt="Exemplo do Elbow Method">
	</section>
	<section data-markdown>
		## Escolhendo K
		- Outra forma de avaliar é identificando se esses clusters apareceriam em novos dados, ou fazendo validação cruzada
		- Existem outras formas de identificar o K como a estatística _gap_ definida por Robert Tibshirani et al.
	</section>
	<section data-markdown>
		- O número de clusters é definido pelo usuário
		- O algoritmo adiciona registros de forma iterativa ao cluster mais próximo
		- __O número de clusters é definido pelo usuário__
	</section>
	<section data-markdown>
		## Clustering Hierárquico
		- É uma alternativa ao Kmeans e acomoda variáveis não numéricas mais facilmente
		- Encontra outliers ou grupos anômalos com mais facilidade
		- Cria uma saída gráfica e mais intuitiva
		- Não escala bem para dados com milhões de registros e pode ser computacionalmente intenso
		- A maior parte de sua aplicabilidade é para pequenos dados
	</section>
	<section>
		<h3>Clustering Hierárquico</h3>
		<ul>
			<li>Funciona em datasets com _n_ registros e _p_ variáveis sendo baseados em duas métricas</li>
			<ol>
				<li>Uma medida de distância $d_{i,j}$ que mede a distância entre os registros $i$ e $j$</li>
				<li>Métrica de dissimilaridade $D_{A,B}$ que mede a diferença entre dois clusters $A$ e $B$, baseado
					na distância $d_{i,j}$
				</li>
			</ol>
			<li>A métrica mais importante é a dissimilaridade, começamos com cada registro como um único cluster 
				e combinamos os menos dissimilares
			</li>
		</ul>
	</section>
	<section>
		<pre><code>
## Hieraquical Clustering
syms1 &lt;- c('GOOGL', 'AMZN', 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 
           'XOM', 'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP',
           'WMT', 'TGT', 'HD', 'COST')

df &lt;- sp500_px[row.names(sp500_px)>='2011-01-01', syms1]
d &lt;- dist(t(df))
hcl &lt;- hclust(d)
		</code></pre>
	</section>
	<section data-markdown>
		## Dendrograma
		- O clustering hierárquico tem um formato natural de árvore conhecido como _dendrograma_
		- Do grego _dendro_ &rarr; árvore e _gramma_ &rarr; desenho
		- `plot(hcl)`
		- [Formas de Desenhar dendrograma](http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning)
		- A altura da árvore indica os graus de dissimilaridade
		- Amazon e Google são bem diferente das outras, já as outras ações são agrupadas por energia, finanças e consumo
	</section>
	<section data-markdown>
		## Clustering Hierárquico
		- Contrário ao K-means, não precisamos indicar o valor de K, é possível fazer usando o comando 
		- `cutree(hcl,k=4)` 
	</section>
	<section data-markdown>
		## Algoritmo Aglomerativo
		- É o algoritmo principal que vai juntando clusters similares, cada registro começa sendo um cluster
		- Para cada par de registros $(x_1,x_2,\dots,x_p)$ e $(y_1,y_2,\dots,y_p)$
		- Usamos a distância euclidiana
		$$ d(x,y) = \sqrt[2]{(x_1 - y_2)^2 + (x_2-y_2)^2 + \dots + (x_p - y_2)^2} $$
		- Agora devemos medir a distância entre clusters
	</section>
	<section data-markdown>
		## Algoritmo Aglomerativo
		- Tendo dos clusters $ A = (a_1, a_2 , \dots, a_m) $ e $ B = (b_1,b_2, \dots, b_q) $
		- Uma medida de dissimilaridade é o _complete-linkage_ que considera a maior distância entre todos pares de A e B
		$$ D(A,B) = max\, d(a_p,b_j) \forall pares\,i,j $$
	</section>
	<section data-markdown>
		## Algoritmo Aglomerativo
		1. Crie um conjunto inicial de clusters onde cada registro é um clusters para todos os registros
		1. Calcule a dissimilaridade $D(C_k,C_l)\, \forall \, pares\,k,l $
		1. Junte os dois clusters $C_k$ e $C_l$ que tiverem a menor dissimilaridade 
		1. Se existir mais de um cluster, volte para 2
	</section>
	<section data-markdown>
		## Medidas de Dissimilaridade
		- Existem 4 medidas comuns de dissimilaridade: _complete-linkage_, _average linkage_, _single linkage_ e
		_minimum variance_ 
		- Essas medidas e outras são suportados pela maioria das funções de clustering hierárquico como hclust
		- O _complete-linkage_ tende a ser o que forma membros mais similares
		- _single linkage_ usa a menor distância e gera clusters com elementos dispares
		- A linkagem média tira a média das distâncias e é um meio termo entre complete e single
		- Já o minimum miniminiza a soma dos quadrados entre os clusters (kmeans)
	</section>
	<section>
		<pre><code>
cluster_fun &lt;- function(df, method)
{
	d &lt;- dist(df)
	hcl &lt;- hclust(d, method=method)
	tree &lt;- cutree(hcl, k=4)
	df$cluster &lt;- factor(tree)
	df$method &lt;- method
	return(df)
}

df0 &lt;- sp500_px[row.names(sp500_px)>='2011-01-01', c('XOM', 'CVX')]
df &lt;- rbind(cluster_fun(df0, method='single'),
	cluster_fun(df0, method='average'),
	cluster_fun(df0, method='complete'),
	cluster_fun(df0, method='ward.D'))
df$method &lt;- ordered(df$method, c('single', 'average', 'complete', 'ward.D'))

ggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +
	geom_point(alpha=.3) +
	scale_shape_manual(values = c(46, 3, 1,  4),
		guide = guide_legend(override.aes=aes(size=2))) +
	facet_wrap( ~ method) +
	theme_bw()
		</code></pre>
	</section>
	<section data-markdown>
		- Começa com "cada um no seu quadrado"
		- Vai juntando os clusters mais similares
		- É mantido o histórico do algoritmo aglomerativo e pode ser visualizado 
		- A distância entre clusters pode ser calculada de formas diferentes 
	</section>
	<section data-markdown>
		## Clustering baseado em modelos
		- Clusters Hierárquico e K-means são baseados em heurísticas e tentam encontrar registros próximos, sem considerar
		modelos de probabilidade
		- Clusters baseados em modelos usam teorias da estatísticas e fornecem formas mais rigorosas
		- Podem identificar grupos similares mas que podem não estar próximos
	</section>
	<section data-markdown>
		## Distribuição Normal
		- A forma mais comum é o método da _distribuição normal multivariada_ 
		- É uma generalização da distribuição normal para $p$ variáveis  $X_1, X_2, \dots, X_p $
		- A distribuição é formada por um conjunto de médias $\mu = \mu_1, \mu_2, \dots, \mu_p $ e a matriz de covariância
		$ \sum $
		- Lembrando que a matriz de covariância tem as variâncias na diagonal principal e a covariância i,j nas outras células
	</section>
	<section><pre><code>
library(ellipse)
mu &lt;- c(.5, -.5)
sigma &lt;- matrix(c(1, 1, 1, 2), nrow=2)
prob &lt;- c(.5, .75, .95, .99) ## or whatever you want
names(prob) &lt;- prob ## to get id column in result
x &lt;- NULL
for (p in prob){
  x &lt;- rbind(x,  ellipse(x=sigma, centre=mu, level=p))
}
df &lt;- data.frame(x, prob=factor(rep(prob, rep(100, length(prob)))))
names(df) &lt;- c("X", "Y", "Prob")

dfmu &lt;- data.frame(X=mu[1], Y=mu[2])

ggplot(df, aes(X, Y)) + 
  geom_path(aes(linetype=Prob)) +
  geom_point(data=dfmu, aes(X, Y), size=3) +
  theme_bw()
	</code></pre></section>
	<section data-markdown>
		## Mistura de normais
		- A ideia do cluster baseado em modelo é assumir que cada registro é distribuído como uma das K distribuições
		multivariadas, onde K é o número de clusters
		- Podemos usar o pacote `mclust`
	</section>
	<section><pre><code>
library(mclust)
df &lt;- sp500_px[row.names(sp500_px)>='2011-01-01', c('XOM', 'CVX')]
mcl &lt;- Mclust(df)
summary(mcl)
	</code></pre></section>
	<section><pre><code>
cluster &lt;- factor(predict(mcl)$classification)

ggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +
	geom_point(alpha=.8) +
	theme_bw() +
	scale_shape_manual(values = c(1, 3),
		guide = guide_legend(override.aes=aes(size=2))) 
	</code></pre>

</section>
<section data-markdown>
	## Model Clustering
	- Podemos extrair os parâmetros da distribuição normal usando `summary(mcl, parameters= TRUE)$mean`
	- `summary(mcl, parameters= TRUE)$variance`
	- A primeira distribuição tem médias e variância similar, já a segunda é muito maior
	- O objetivo é encontrar o conjunto de distribuições normais multivariadas que encaixa melhor
	- Contrário do Kmeans e hierárquivo, mclust seleciona o número de clusters
	- É escolhido o número de clusters para que o BIC (_Bayesian Information Criteria_) tenha o maior valor
</section>
<section data-markdown>
	## Model Clustering 
	- Lembrem que como AIC o BIC é usado para regressão passo a passo
	- BIC encontra o modelo melhor ajustado com penalidade para número de parâmetros no modelo
	- No caso de clustering, aumentar o número de clusters sempre melhora o ajuste mas adiciona parâmetros ao modelo
	- `plot(mcl, what='BIC', ask=FALSE)` 
	- O plot lembra o _elbow plot_ , são 14 linhas pois são ajustados 14 modelos e o melhor é escolhido
</section>
<section data-markdown>
	## Model Clustering
	- São criados vários modelos pois existem diferentes formas de parametrizar a matriz de covariância para
	ajustar o modelo
	- Model-based é uma área que está sendo muito desenvolvida e tem diversas formas de usar
	- Talvez os detalhes do model-based sejam mais esforço do que o necessário para data science 
	- É computacionalmente caro e depende de assumir um modelo para os dados 
</section>
<section data-markdown>
	- Assumem que os clusters derivam de diferentes distribuições de probabilidade
	- Diversos modelos são encaixados, assumindo diferentes números de distribuições
	- O método escolhe o modelo que encaixa bem os dados sem muitos parâmetros
</section>
<section data-markdown>
	## Escala e Categorias
	- Técnicas não supervisionadas costumam precisas que os dados sejam escalados
	- Como vimos no caso do loan_data, temos variáveis com unidades e magnitudes diferentes, isso é um problemas para
	PCA, K-means e outros métodos de clustering
	- Outro problemas são variáveis categóricas, se usarmos _one-hot encoder_ os dados só terão dois valores, o que pode
	ser problemático
</section>
<section>
	<pre><code>
defaults &lt;- loan_data[loan_data$outcome=='default',]
df &lt;- defaults[, c('loan_amnt', 'annual_inc', 'revol_bal', 'open_acc', 'dti', 'revol_util')]
km &lt;- kmeans(df, centers=4, nstart=10)
centers &lt;- data.frame(size=km$size, km$centers) 
round(centers, digits=2)
			
	</code></pre>
	<p> annual_inc e revol_bal dominam os clusters, notem também a diferença no número de elementos por cluster</p>
</section>
<section data-markdown>
	## Escalando
	- Uma técnica já vista é transformar as variáveis em escore-z, subtraindo a média e dividindo pelo desvio padrão
	$$ z = \frac{x-\hat{x}}{s} $$
	- O escalonamento também é importante para PCA, fazer isso é equivalente a usar a matrix de correlação no lugar
	da de covariância, algumas funções dão essa opção (usar a matriz de correlação)
	- No `princomp` temos o argumento `cor`
</section>
<section>
	<pre><code>
df0 &lt;- scale(df)
km0 &lt;- kmeans(df0, centers=4, nstart=10)
centers0 &lt;- scale(km0$centers, center=FALSE, scale=1/attr(df0, 'scaled:scale'))
centers0 &lt;- scale(centers0, center=-attr(df0, 'scaled:center'), scale=FALSE)
centers0 &lt;- data.frame(size=km0$size, centers0) 
round(centers0, digits=2)

km &lt;- kmeans(df, centers=4, nstart=10)
centers &lt;- data.frame(size=km$size, km$centers) 
round(centers, digits=2)
	</code></pre>
</section>
<section data-markdown>
	## Variáveis Dominante
	- Mesmo em casos que as variáveis são medidas na mesma escala e informam de forma precisa sua importância
	pode ser útil reescalar as variáveis
	- Se a gente adicionar GOOGL e AMZN à análise do PCA vemos que elas dominam os dois primeiros componentes
	- Isso acontece porque a movimentação de preço dessas duas ações dominam a variabilidade
	- Podemos manter, reescalar ou então eliminar a variável dominante e tratar de forma separada
</section>
<section>
	<pre><code>
syms <- c('GOOGL', 'AMZN', 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 
	'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')
  top_15 <- sp500_px[row.names(sp500_px)>='2011-01-01', syms]
  sp_pca1 <- princomp(top_15)
  
screeplot(sp_pca1, main='')
  
round(sp_pca1$loadings[,1:2], 3)
	</code></pre>
</section>
<section data-markdown>
	### Dados Categóricos e distância de Gower
	- Para dados categóricos devemos converter eles para valores numéricos seja através de ranking no cado de fatores
	ordenados ou usando variáveis dummy
	- Quando isso acontece é interessante escalar os dados e um método popular é a distância de gower
		- Para variáveis numéricas e fatores ordenadas a distância é calculada como o valor absoluto da distância dos registros (Manhattan) 
		- Para variáveis categóricas a distância é 1 se as categorias são diferentes e 0 se são iguais
</section>
<section data-markdown>
	## Distância Gower
	1. Compute a distância $d_{i,j} \forall \, i,j \, pares $
	1. Escale cada par $d_{i,j} $ de forma que o menor seja 0 e o maior 1
	1. Some as distâncias escaladas entre as variáveis calculando uma média ou média ponderada
</section>
<section>
	<pre><code>
x 7lt;- loan_data[1:5, c('dti', 'payment_inc_ratio', 'home_', 'purpose_')]
x

daisy(x, metric='gower')			
	</code></pre>
	<p>Todas as distâncias ficam entre 0 e 1, o par com maior distância é 2,3 e não compartilha home e purpose,
		além de valores diferentes de dti. Já 3 e 5 são os mais próximos e possuem mesmo home e purpose
	</p>
</section>
<section>
	<pre><code>
set.seed(301)
df &lt;- loan_data[sample(nrow(loan_data), 250),
				c('dti', 'payment_inc_ratio', 'home_', 'purpose_')]
d = daisy(df, metric='gower')
hcl &lt;- hclust(d)
dnd &lt;- as.dendrogram(hcl)

par(mar=c(0,5,0,0)+.1)
plot(dnd, leaflab='none', ylab='distance')
	</code></pre>
</section>
<section><pre><code>
dnd_cut &lt;- cut(dnd, h=.5)
df[labels(dnd_cut$lower[[2]]),]
</code></pre>
 <p>Todos tem home e purpose iguais! Isso ilustra que variáveis categóricas tendem a ser agrupadas nos clusters</p>
</section>
<section data-markdown>
	## Problemas com Clustering
	- K-means e PCA são mais apropriados para variáveis contínuas
	- Para poucos dados hierárquico com distância de Gower é uma boa técnica
	- Poderíamos usar _one hot encoder_ mas vimos que PCA e K-means têm dificuldade com variáveis binárias
	- Se usarmos Escore-Z as variáveis binárias vão dominar os clusters
	- Poderíamos escalar as categóricas para reduzir seu impacto ou então fazer clusters separados de cada motivo de 
	empréstimo
</section>
<section>
	<pre><code>
df &lt;- model.matrix(~ -1 + dti + payment_inc_ratio + home_ + pub_rec_zero, data=defaults)
df0 &lt;- scale(df)
km0 &lt;- kmeans(df0, centers=4, nstart=10)
centers0 &lt;- scale(km0$centers, center=FALSE, scale=1/attr(df0, 'scaled:scale'))
round(scale(centers0, center=-attr(df0, 'scaled:center'), scale=FALSE), 2)
	</code></pre>
</section>
<section data-markdown>
		- Variáveis em escalas diferentes devem ser transformadas para a mesma escala
		- Podemos usar a normalização para escalar
		- Outro método é a distância de Gower 
	</section>
	<section data-markdown>
		- Para redução de dimensionalidade as ferramentas principais são PCA e K-means, ambas devem ser escaladas
		- Para clusters altamente estruturados, todos métodos vão produzir resultados parecidos
		- K-means escala bem para grandes quantidades de dados
		- Hierárquico pode ser aplicado para dados de tipos misturados (categóricos e numéricos) além de gerar o dendrograma
		- Em dados com muitos ruídos a escolha é mais difícil com cada um gerando soluções bem diferentes
		- Não existe uma regra do que fazer, vai depender do objetivo e tamanho dos dados
	</section>
				</div>
		</div>

		<script src="../lib/js/head.min.js"></script>
		<script src="../js/reveal.js"></script>
		<script src="../js/base.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: 'c/t',

				transition: 'slide', // none/fade/slide/convex/concave/zoom
				math: {
					// mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					config: 'TeX-AMS_HTML-full',
					TeX: {
						Macros: {
							R: '\\mathbb{R}',
							set: [ '\\left\\{#1 \\; ; \\; #2\\right\\}', 2 ]
						}
					}
				},
				// Optional reveal.js plugins
				dependencies: [
          { src: '../lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: '../plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: '../plugin/zoom-js/zoom.js', async: true },
					{ src: '../plugin/notes/notes.js', async: true },
					{ src: '../plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
