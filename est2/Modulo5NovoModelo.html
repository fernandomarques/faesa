<!doctype html>
<html lang="pt">

	<head>
		<meta charset="utf-8">

		<title>Estatística para Data Science II</title>

		<meta name="author" content="Fernando Antonio Marques Filho">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="../css/reveal.css">
		<link rel="stylesheet" href="../css/misc.css">
		<link rel="stylesheet" href="../css/theme/white.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="../lib/css/zenburn.css">
		<style>
			img {
				border : 0px !important;
			}
			h2, h3 {
				font-family: Arial !important;
				color: #17365d !important;
				font-weight: bold !important;
			}
			p, li{
				font-family: Arial !important;
				color: #1f497d !important;
				font-weight: bold !important;
			}
			.logo {
				position: absolute;
				left: 0px;
				height: 100%;
			}
		</style>
		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? '../css/print/pdf.css' : '../css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>
	<body>
		<img class="logo" src="capa_pos/logo_vertical.png">
		<div class="reveal">
			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section data-background-image="capa_pos/capa_topico_1.png">
					<!-- <h3>Árvores Aleatórias e Boosting</h3>	
					<img src="./img/logopos.webp" alt="">
					<p>
						<small>Created by Fernando Marques </small>
					</p> -->
		</section>
		<section data-markdown="">
			> Em 1906 o estatístico Sir Francis Galton visitava uma feira na Inglaterra.
			Os participantes deveriam adivinhar o peso de um touro adornado que estava em exposição.
			Foram 800 tentativas e apesar dos valores individuais variarem muito, tanto a média
			quanto a mediana ficaram a 1% do peso real do animal.			

		</section>
	<section data-markdown="">
		## Ensemble
		- Ensemble, formar uma predição com base em vários modelos, é usado em diferentes modelos
		- A versão simples funciona da seguinte forma
			1. Crie um modelo preditivo e registre as predições para os dados
			1. Repita para vários modelos nos mesmo dados
			1. Para cada registro predito calcule a média (ou média pondera, ou maioria dos votos)
		- Para árvores existem duas variações mais comuns:
			1. Bagging, que em árvore é random forest
			1. Boosting
	</section>
	<section data-markdown="">
		## Bagging
		- Vem de "bootstrap aggregating" foi introduzido em 1994, suponha que tenhamos uma resposta Y com P preditores
		X = X1, X2, ... , Xp com N registros
		- Ao invés de criar vários modelos do mesmo dado, cada modelo é ajustado a uma reamostra de bootstrap
			1. Inicialize M, número de modelos e n (n&lt;N), número de registros. m = 1
			1. Tire uma amostra do bootstrap de tamanho n para formar Ym e Xm
			1. Treine o modelo usando Ym e Xm para criar uma regra fm(X)
			1. Incremente o contador m = m + 1. Se m &lt; M, volte para 2.
	</section>
	<section data-markdown="">
		## Bagging
		- Depois de terminar o algoritmo teremos a fórmula
		\begin{aligned}
		\hat{f} = \frac{1}{M}(\hat{f}_1(X) + \hat{f}_2(X) + \dots + \hat{f}_M(X))
		\end{aligned}
	</section>
	<section data-markdown="">
		## Floresta Aleatória
		- Random Forest é um trademark!
		- A floresta aleatória aplica o bagging a árvores com uma extensão:
			- Além de fazer amostragem dos dados, também faz amostragem das variáveis preditoras
		- Em árvores tradicionais o algoritmo cria subpartições minimizando a impureza com critérios como Gini Impurity
		- Na floresta aleatória a cada estágio o algoritmo escolha variáveis de um subset aleatório de variáveis
		- Comparado ao algoritmo da árvore é adiciona um passo para o bagging e bootstrap das variáveis
	</section>
	<section data-markdown="">
		## Floresta Aleatória
		- Mas então quantas variáveis devemos ter para cada passo? Uma regra é usar &radic;P onde P é o número de preditores
		- O pacote `randomForest` implementa florestas aleatórias
		- Por padrão são treinadas 500 árvores
		- A estimativa _out-of-bag_ (OOB) estima a taxa de erro para os modelos treinados, usando dados que não foram usados no treinados
		- A taxa de erro caí rápidamente até estabilizar
	</section>
	<section>
		<pre><code>
loan3000 &lt;- read.csv('loan3000.csv')
loan3000$outcome &lt;- ordered(loan3000$outcome, levels=c('paid off', 'default'))
library(randomForest)
rf &lt;- randomForest(outcome ~ borrower_score + payment_inc_ratio,
	data=loan3000)

rf
		</code></pre>
	</section>
	<section>
		<pre><code>
error_df = data.frame(error_rate = rf$err.rate[,'OOB'],
	num_trees = 1:rf$ntree)
ggplot(error_df, aes(x=num_trees, y=error_rate)) +
geom_line()  +
theme_bw()
		</code></pre>
	</section>
	<section>
		<pre><code>
pred &lt;- predict(rf, prob=TRUE)
rf_df &lt;- cbind(loan3000, pred = pred)

ggplot(data=rf_df, aes(x=borrower_score, y=payment_inc_ratio, 
	shape=pred, color=pred)) +
geom_point(alpha=.6, size=2) +
scale_shape_manual( values=c( 46, 4)) +
scale_x_continuous(expand=c(0,0)) + 
scale_y_continuous(expand=c(0,0), lim=c(0, 20)) + 
theme_bw()
		</code></pre>
	</section>
	<section data-markdown="">
		## Random Forest
		- O método funciona como uma caixa preta
		- Apesar de produzir predições mais precisas que árvores, as regras intuítivas das árvores são perdidas
		- Reparem que alguns empréstimos com um score alto foram marcados como inadimplentes, pode ser resultado
		de registros incomuns e mostra os perigos de overfitting com florestas aleatórias
	</section>
	<section data-markdown="">
		## Variáveis de Importância
		- O poder da floresta aleatória aparece quando criamos um modelo com muitos preditores
		- Ela tem a habilidade de determinar quais preditores são mais importantes e encontrar relações complexas

	</section>
	<section>
		<pre><code>
library(dplyr)
loan_data &lt;- read.csv('loan_data.csv')
loan_data &lt;- select(loan_data, -X, -status)
			
rf_all &lt;- randomForest(outcome ~ ., data=loan_data, importance=TRUE)
rf_all

varImpPlot(rf_all, type=1)


		</code></pre>
	</section>
	<section>
		<pre><code>
imp1 &lt;- importance(rf_all, type=1)
imp2 &lt;- importance(rf_all, type=2)
idx &lt;- order(imp1[,1])
nms &lt;- factor(row.names(imp1)[idx], levels=row.names(imp1)[idx])
imp &lt;- data.frame(Predictor = rep(nms, 2),
	Importance = c(imp1[idx, 1], imp2[idx, 1]),
	Type = rep( c('Accuracy Decrease', 'Gini Decrease'), rep(nrow(imp1), 2)))
ggplot(imp) + 
geom_point(aes(y=Predictor, x=Importance), size=2, stat="identity") + 
facet_wrap(~Type, ncol=1, scales="free_x") + 
theme(
	panel.grid.major.x = element_blank() ,
	panel.grid.major.y = element_line(linetype=3, color="darkgray") ) +
theme_bw()
		</code></pre>
	</section>
	<section data-markdown="">
		![Output da RandomForest](img/Modulo5OutputArvore.png)
	</section>
	<section data-markdown="">
		![Output da RandomForest](img/Modulo5Importancia.png)
	</section>
	<section data-markdown="">
		## Variáveis de Importância
		- Existem duas formas de medir a importância das variáveis
			1. A diminuição da acurácia do modelo quando as variáveis são permutadas
			1. Pela diminuição do valor do Gini Impurity Score
		- Por padrão o modelo calcula Gini como subproduto do algoritmo, enquanto a acurácia requer computação extra
		- Em casos onde a complexidade computacional é muito alta, o Gini pode ser suficiente 
		- O Gini também mostra quais variáveis estão sendo sendo mais usadas para fazer o particionamento
	</section>
	<section data-markdown="">
		## Hiperparâmetros
		- Assim como muitos algoritmos de aprendizagem de máquina a floresta aleatória pode ser vista como uma caixa preta
		- Mas é uma caixa preta com botões que podem ajustar suas funcionalidades
		- Os Hiperparâmetros são especialmente importantes para evitar overfitting
		- Os dois Hiperparâmetros mais importantes são:
			- `nodesize`,  menor número possível para as folhas. O padrão é 1 para classificação e 5 para regressão
			- `maxnodes`, máximo de nós em uma árvore de decisão, não tem limite por padrão e é limitado por nodesize
	</section>
	<section data-markdown="">
		- Modelos "ensemble" melhoram a acurácia combinando o resultado de vários modelos
		- Bagging é um tipo particular de ensemble que usa bootstrap para ajustar vários modelos
		- Random forest é um tipo especial de bagging que alem de amostras dos dados também faz amostras dos preditores
		- Um resultado importante da floresta aleatória é a importância dos preditores
		- A floresta aleatória possui Hiperparâmetros que podem ser usados com avaliação cruzada para evitar overfitting
	</section>
	<section data-markdown="">
		## Boosting
		- Boosting é uma técnica para criar modelos ensemble e também é usado primariamente em árvores de decisão
		- Comparado ao bagging é mais poderoso mas requer maior cuidado
		- Boosting encaixe uma série de modelos, com cada modelo sucessivo para minimizar o erro do modelo anterior
		- Existem muitos algoritmos como Adaboot, gradient boosting, stochastic gradient boosting, sendo o último o mais usado
	</section>
	<section data-markdown="">
		## Boosting Adaboost
		1. Inicialize M, quantidade de modelos. m = 1, inicie os pesos de observação $w_i = 1/N$ para i = 1,2,3,...,N
		1. Treine o modelos usando $\hat{f}_m$ com os pesos que minimizem o erro
		1. Adicione o modelo ao arranjo $\hat{F}m = $ $\hat{F}_{m-1} + \alpha_m \hat{f}_m $ onde $ \alpha_m = \frac{log1 - e_m}{e_m}$
		1. Atualize os pesos w aumentado para observações classificadas erradas
		1. Incremente o contador do modelo
			- O modelo é formado por $$ \hat{F} = \alpha_1\hat{f}_1 + \alpha_2\hat{f}_2 + \dots + \alpha_M\hat{f}_M $$
	</section>
	<section data-markdown="">
		## Boosting Adaboost
		- Os pesos são aumentados para observações erradas, forçando o algoritmo a treinar mais com os dados que se saiu pior

	</section>
	<section data-markdown="">
		## XGBoost
		- O software mais utilizado de domínio público é o XGBoost, uma implementação do gradient estocástico
		- Uma implementação eficiênte está disponível no pacote xgboost
		- Os parâmetros da função podem e devem ser ajustados, sendo os mais importantes:
			- `subsample` que controla a fração da observação que deve ser amostrada
			- `eta` um fator de encolhimento aplicado ao $\alpha_m$, esse fator previne overfitting diminuindo a mudança
			dos pesos			
	</section>
	<section><pre><code>
library(xgboost)
predictors &lt;- data.matrix(loan3000[, c('borrower_score', 'payment_inc_ratio')])
label &lt;- as.numeric(loan3000[,'outcome'])-1
xgb &lt;- xgboost(data=predictors, label=label, objective = "binary:logistic", 
	params=list(subsample=.63, eta=0.1), nrounds=100)

	</code></pre>
	<p>XGBoost não aceita a sintaxe de fórmula, por isso, temos que converter os preditores em data.matrix e a 
		resposta em variáveis 0/1
	</p>
</section>
<section><pre><code>
pred <- predict(xgb, newdata=predictors)
xgb_df <- cbind(loan3000, pred_default=pred>.5, prob_default=pred)

ggplot(data=xgb_df, aes(x=borrower_score, y=payment_inc_ratio, 
color=pred_default, shape=pred_default)) +
  geom_point(alpha=.6, size=2) +
  scale_shape_manual( values=c( 46, 4)) +
  scale_x_continuous(expand=c(.03, 0)) + 
  scale_y_continuous(expand=c(0,0), lim=c(0, 20)) + 
  theme_bw()
</code></pre></section>
	<section data-markdown="">
		## Evitando overfitting
		- O uso às cegas do xgboost pode acarretar em modelos instáveis como resultado do overfit
		- Com overfit o acurácia do modelo para dados não treinados será ruim
		- As predições serão altamente variáveis
	</section>
	<section>
		<pre><code>
seed <- 400820
predictors <- data.matrix(loan_data[,-which(names(loan_data) %in% 'outcome')])
label <- as.numeric(loan_data$outcome)-1
test_idx <- sample(nrow(loan_data), 10000)

xgb_default <- xgboost(data=predictors[-test_idx,], label=label[-test_idx], 
                       objective = "binary:logistic", nrounds=250, verbose=0)
pred_default <- predict(xgb_default, predictors[test_idx,])
error_default <- abs(label[test_idx] - pred_default) > 0.5
xgb_default$evaluation_log[250,]
mean(error_default)
		</code></pre>
		<p>Apesar de taxa de erro de 14.8% no modelo treinado, a taxa de erro no teste é 36.2%!</p>
	</section>
	<section data-markdown="">
		## Evitando Overfitting
		- Além dos parâmetros também podemos alterar a função de custo de forma a penalizar a complexidade do modelo
		- Existem dois parametros para isso alpha e lambda que correspondem as distâncias manhattan e euclidiana
	</section>
	<section>
		<pre><code>
xgb_penalty <- xgboost(data=predictors[-test_idx,], 
	label=label[-test_idx], 
	params=list(eta=.1, subsample=.63, lambda=1000),
	objective = "binary:logistic", nrounds=250, verbose=0)
pred_penalty <- predict(xgb_penalty, predictors[test_idx,])
error_penalty <- abs(label[test_idx] - pred_penalty) > 0.5
xgb_penalty$evaluation_log[250,]
mean(error_penalty)
		</code></pre>
	</section>
	<section>
		<pre><code>
error_default <- rep(0, 250)
error_penalty <- rep(0, 250)
for(i in 1:250)
{
	pred_default <- predict(xgb_default, predictors[test_idx,], ntreelimit = i)
	error_default[i] <- mean(abs(label[test_idx] - pred_default) > 0.5)
	pred_penalty <- predict(xgb_penalty, predictors[test_idx,], ntreelimit = i)
	error_penalty[i] <- mean(abs(label[test_idx] - pred_penalty) > 0.5)
}

errors <- rbind(xgb_default$evaluation_log,
				xgb_penalty$evaluation_log,
				data.frame(iter=1:250, train_error=error_default),
				data.frame(iter=1:250, train_error=error_penalty))
errors$type <- rep(c('default train', 'penalty train', 
						'default test', 'penalty test'), rep(250, 4))

ggplot(errors, aes(x=iter, y=train_error, group=type)) +
	geom_line(aes(linetype=type, color=type), size=1) +
	scale_linetype_manual(values=c('solid', 'dashed', 'dotted', 'longdash')) +
	theme_bw() +
	theme(legend.key.width = unit(1.5,"cm")) +
	labs(x="Iterations", y="Error") +
	guides(colour = guide_legend(override.aes = list(size=1)))
</code></pre>
	</section>
	<section>
		<h3>Hiperparametros e Validação cruzada</h3>
		<pre><code>
N <- nrow(loan_data)
fold_number <- sample(1:5, N, replace = TRUE)
params <- data.frame(eta = rep(c(.1, .5, .9), 3),
						max_depth = rep(c(3, 6, 12), rep(3,3)))
rf_list <- vector('list', 9)
error <- matrix(0, nrow=9, ncol=5)
for(i in 1:nrow(params)){
	for(k in 1:5){
	cat('Fold', k, 'for model', i, '\n')
	fold_idx <- (1:N)[fold_number == k]
	xgb <- xgboost(data=predictors[-fold_idx,], label=label[-fold_idx], 
					params = list(eta = params[i, 'eta'], 
									max_depth = params[i, 'max_depth']),
					objective = "binary:logistic", nrounds=100, verbose=0)
	pred <- predict(xgb, predictors[fold_idx,])
	error[i, k] <- mean(abs(label[fold_idx] - pred) >= 0.5)
	}
}

avg_error <- 100 * round(rowMeans(error), 4)
cbind(params, avg_error)
		</code></pre>
		<p>Para esse data é sugerido eta=0.1 e max_depth=3</p>
	</section>
	<section data-markdown="">
		## Hiperparâmetros XGBoost
		- `eta` fator de encolhimento de 0 a 1 para &alpha;. Padrão é 0.3, para dados pequenos é sugerido 0.1
		- `nrounds` número de rodadas de boosting, se eta for pequeno ele deve ser grande pois a aprendizagem será mais lenta
		- `max_depth` maior profundidade da árvore (padrão 6) contrário de random forest, xgboot tem árvores shallow
		- `subsample colsample_bytree` - fração dos registros para serem amostrados sem reposição
		- `lambda e alpha` parâmetros de regularização
	</section>
	<section data-markdown="">
		- Boosting é uma classe de modelos arranjados (ensemble) que ajusta uma sequência de modelos dando peso maior
		a registro com erros
		- _Stochastic gradient boosting_ é o boosting mais usado
		- Também pode sofrer de overfitting, por isso os parâmetros
		- Validação cruzada é muito importante para XGBoost devido a grande variedade de Hiperparâmetros
	</section>
				</div>
		</div>

		<script src="../lib/js/head.min.js"></script>
		<script src="../js/reveal.js"></script>
		<script src="../js/base.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: 'c/t',

				transition: 'slide', // none/fade/slide/convex/concave/zoom
				math: {
					// mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					config: 'TeX-AMS_HTML-full',
					TeX: {
						Macros: {
							R: '\\mathbb{R}',
							set: [ '\\left\\{#1 \\; ; \\; #2\\right\\}', 2 ]
						}
					}
				},
				// Optional reveal.js plugins
				dependencies: [
          { src: '../lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: '../plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: '../plugin/zoom-js/zoom.js', async: true },
					{ src: '../plugin/notes/notes.js', async: true },
					{ src: '../plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
