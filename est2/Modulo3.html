<!doctype html>
<html lang="pt">

	<head>
		<meta charset="utf-8">

		<title>Estatística para Data Science II</title>

		<meta name="author" content="Fernando Antonio Marques Filho">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="../css/reveal.css">
		<link rel="stylesheet" href="../css/misc.css">
		<link rel="stylesheet" href="../css/theme/sky.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="../lib/css/zenburn.css">
		<style>
			img {
				border : 0px !important;
			}
		</style>
		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? '../css/print/pdf.css' : '../css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>
	<body>
		<div class="reveal">
			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h3>Classificação</h3>	
					<img src="./img/logopos.webp" alt="">
					<p>
						<small>Created by Fernando Marques </small>
					</p>
		</section>
		<section data-markdown="">
			## Classificação
			- Muitas vezes queremos responder há algumas perguntas de forma automática
				- É span? O usuário vai clicar no anúncio?
			- A Classificação pode ser binária como visto, ou então categórica
			- No gmail, um email pode ser primary, social, promotional, forum...
			- Muitas vezes queremos mais que uma resposta binária, qual a probabilidade de ser verdadeiro ou falso?
		</section>
		<section data-markdown="">
			## Classificação
			- Ao invés de retornar um valor binário, a maioria dos algoritmos retornam uma pontuação
			- É escolhido um limite, se o valor estiver acima do limite, é considerado dentro da categoria
		</section>
		<section data-markdown="">
			## Probabilidade Condicional
			- Probabilidade de observar um evento B dado que um outro evento A já aconteceu
			\begin{aligned}
			P(B|A)
			\end{aligned}
			- Quem comprou X também comprou Y
			- P(A,B) é a probabilidade de A e B acontecerem
			\begin{aligned}
			P(B|A) = \frac{P(A,B)} {P(A)}
			\end{aligned}
		</section>
		<section data-markdown="">
			## Probabilidade Condicional
			- Foi passado um teste para os alunos
			- 60% passou nos dois testes
			- Como o primeiro teste foi mais fácil, 80% passou nele
			- Qual porcentagem que passou no primeiro também passou segundo teste?
			\begin{aligned}
			P(B|A) = \frac{P(A,B)} {P(A)} = \frac{0.6}{0.8} = 0.75
			\end{aligned}
			- Se A e B fossem independentes, P(B|A) seria próximo de P(B)
		</section>
		<section data-markdown="">
			## Teorema de Bayes
			\begin{aligned}
			P(A|B) = \frac{P(A)P(B|A)} {P(B)} 
			\end{aligned}
			- A probabilidade de algo que depende de B, depende das probabilidades básicas de A e B
			- A probabilidade de detectar um usuário de drogas é alto, mas não necessáriamente significa que a chance de ser usuário de drogas é alto já que o teste deu positivo
		</section>
		<section data-markdown="">
			## Teorema de Bayes
			- Testes de drogas com acurácia boa podem gerar mais falsos positivos que verdadeiros positivos
			- Um teste de droga identifica um usuário com 99% de chance
			- E tem 99% de chance de dar negativo com não usuários
			- Mas apenas 0.3% da população usa a droga
			\begin{aligned}
			P(A|B) = \frac{P(A)P(B|A)} {P(B)} =  \frac{0.003*0.99} {P(0.013)}
			\end{aligned}
		</section>
		<section data-markdown="">
			## Teorema de Bayes
			- A &rarr; é usuário da droga
			- B &rarr; a droga deu positiva
			- P(B) = 0.99x0.003 + 0.01 x 0.997 = 0.013 
			- P(B) é probabilidade de positivo se usa e positivo se não usa
			\begin{aligned}
			P(A|B) = \frac{P(A)P(B|A)} {P(B)} =  \frac{0.003*0.99} {P(0.013)} = 0.228
			\end{aligned}
			- A probabilidade de ser usuário é de 22.8%!
		</section>
		<section data-markdown="">
			## Naive Bayes
			- Usa a probabilidade de observar um valor de preditor, dado o resultado para estimar a probabilidade de observar o resultado dado os preditores
		</section>
		<section data-markdown="">
			## Non-naive Bayes
			- Para comparar, imagine esse classificador não ingênuo
				1. Encontre todos os registros com o mesmo perfil de preditores (iguais)
				1. Determine a qual classe eles pertencem e qual classe é mais provável
				1. Atribua a classe ao novo registro
			- Esse tipo de classificador só funciona se encontrar perfis iguais
			- Dá para perceber que vai ser difícil encontrar registros com os mesmo valores se tivermos alguns preditores
			- Uma casa, reformada ano passado, com 3 quartos e dois banheiros, de frente para o mar...
		</section>
		<section data-markdown="">
			## Naive Bayes
			- Apesar do nome, não é um método bayesiano da estatística
			- Não olhamos apenas para registros iguais ao que será previsto
				1. Para uma resposta binária Y = i, estimar a probabilidade de cada preditor P(X|Y=i), feita usando proporção na <base href="">
				1. Multiplica as probabilidades e multiplicar a proporção de valores Y = i
				1. Repita 1 e 2 para todas as classes
				1. Estimar a probabilidade de i, pegando o valor do passo 2 e dividido pela soma dos valores para todas as classes
				1. Definir o valor para a classe com maior probabilidade
		</section>
		<section data-markdown="">
			## Naive Bayes
			\begin{aligned}
			P(X_1,X_2,...,X_p) = P(Y=0)(P(X_1|Y=0)P(X_2|Y=0)
			\end{aligned}
			\begin{aligned}
			...P(X_p|Y=0)) + P(Y=1)...
			\end{aligned}
			- É considerado ingênuo pois estamos considerando que as variáveis preditoras são independentes
		</section>
		<section >
<pre><code>
library(klaR)
naive_model &lt;- NaiveBayes(outcome ~ purpose_ + home_ + emp_len_, 
                          data = na.omit(loan_data))
naive_model$table

</code></pre>
<p>Os resultados são as probabilidades P(Xj|Y=i)</p>
		</section>
		<section >
<pre><code>
new_loan &lt;- loan_data[147, c('purpose_', 'home_', 'emp_len_')]
row.names(new_loan) &lt;- NULL
new_loan

predict(naive_model, new_loan)

</code></pre>
<p>Também mostra a probabilidade</p>
<p>Esse classificador conhecidamente gera estimativas enviesadas, mas quando o objetivo
	é ordenar os registros pela probabilidade de Y=1 ele gera bons resultados
</p>
		</section>
		<section >
<pre><code>
less_naive &lt- NaiveBayes(outcome ~ borrower_score + payment_inc_ratio + 
		purpose_ + home_ + emp_len_, data = loan_data)
less_naive$table[1:2]

stats &lt- less_naive$table[[1]]
ggplot(data.frame(borrower_score=c(0,1)), aes(borrower_score)) +
	stat_function(fun = dnorm, color='blue', linetype=1, 
	args=list(mean=stats[1, 1], sd=stats[1, 2])) +
	stat_function(fun = dnorm, color='red', linetype=2, 
	args=list(mean=stats[2, 1], sd=stats[2, 2])) +
	labs(y='probability')

</code></pre>
<p></p>
		</section>
		<section data-markdown="">
			## Preditores numéricos
			- Pela definição do classificado bayesiano ele só aceita preditores categóricos
			- Existem duas soluções para aplicar esse classificador com preditores numéricos
				1. Agrupar valores numéricos em preditores categóricos, como fizemos com ZipCode
				1. Usar um modelo de probabilidade, como a distribuição normal, para estimar P(Xj|Y=i)
		</section>
		<section data-markdown="">
			- Naive Bayes funciona bem com preditores categóricos
			- Responde a pergunta "Dentre as categorias de resultado, quais categorias de preditores são mais prováveis"
			- A informação é invertida para estimar a probabilidade de uma categoria resultado, dado os valores do preditor
		</section>
		<section data-markdown="">
			## Análise Discriminante
			-  A análise de discriminante é um dos classificadores estatísticos mais antigos, de 1936 por R.A. Fisher
			- Covariância é uma medida para saber como uma variável varia com relação a outra
			- [Covariance vs. Correlation](https://www.linkedin.com/pulse/covariance-vs-correlation-kumar-p)
			- Apesar da análise de discriminante incluir muitas técnicas a mais comum é a análise de discriminante linear (LDA)
			- LDA é menos usado com o surgimento de técnicas como árvores e regressão logística
			- Ajuda a encontrar preditores mais importantes e é uma medida eficiente para seleção de características
		</section>
		<section data-markdown="">
			## Análise de Discriminante
			- O primeiro artigo de LDA foi em um journal de eugenics
			- Não confundir LDA Linear Discriminant Analysis com LDA Latent Dirichlet Allocation
			- O segundo é usado para processamento de linguagem natural e não são relacionados
		</section>
		<section data-markdown="">
			## LDA - Matriz Covariância
			- Covariância mede a relação entre duas variáveis x e z
			\begin{aligned}
			s_{x,z} = \frac{\sum{(x_i - \bar{x})(z_i - \bar{z})}}{n-1}
			\end{aligned}
		</section>
		<section>

			<table>
				<tr><td>s²<sub>x</sub></td><td>s <sub>x,z</sub></td></tr>
				<tr><td>s <sub>z,x</sub></td><td>s²<sub>z</sub></td></tr>
			</table>
		</section>
		<section data-markdown="">
			## Discriminante Linear de Fisher
			- Para simplicidade vamos classificar um resultado binário y usando duas variáveis numéricas e contínuas x e z
			- Em teoria assume que as variáveis são normalmente distribuídas mas também funciona em casos não extremos
			- O discriminante de fisher diferencia variação entre grupos e dentre os grupos 
			- LDA foca em maximizar a soma dos quadrados entre os grupos em relação a soma dos quadrados dentre os grupos
			
		</section>
		<section data-markdown="">
			## Discriminante Linear de Fisher
			\begin{aligned}
			\frac{SS_{between}}{SS_w}
			\end{aligned}
			- A soma dos quadrados __entre__ é o quadrado da distância entre a média dos dois grupos
			- A soma dos quadrados __dentre__ é a dispersão em torno da média dentro de cada grupo
			- Ponderados pela matriz de covariância
			- Maximizando a soma dos quadrados entre, esse método consegue boa separação entre os grupos
		</section>
		<section>
			<h2>Exemplo</h2>
			<pre><code>
loan_lda &lt;- lda(outcome ~ borrower_score + payment_inc_ratio,
	data=loan3000)
loan_lda$scaling

pred &lt;- predict(loan_lda)
head(pred$posterior)
			</code></pre>
		</section>
		<section>
			<h2>Exemplo</h2>
			<pre><code>
lda_df &lt;- cbind(loan3000, prob_default=pred$posterior[,'default'])
x &lt;- seq(from=.33, to=.73, length=100)
y &lt;- seq(from=0, to=20, length=100)
newdata &lt;- data.frame(borrower_score=x, payment_inc_ratio=y)
pred &lt;- predict(loan_lda, newdata=newdata)
lda_df0 &lt;- cbind(newdata, outcome=pred$class)

ggplot(data=lda_df, aes(x=borrower_score, y=payment_inc_ratio, color=prob_default)) +
	geom_point(alpha=.6) +
	scale_color_gradient2(low='white', high='blue') +
	scale_x_continuous(expand=c(0,0)) + 
	scale_y_continuous(expand=c(0,0), lim=c(0, 20)) + 
	geom_line(data=lda_df0, col='green', size=2, alpha=.8) +
	theme_bw()
			</code></pre>
		</section>
		<section data-markdown="">
			## LDA
			- Quando mais longe o valor da linha, maior o nível de confiança
			- LDA também funciona com mais de duas variáveis preditoras, mas precisa de um certo número de registros para fazer a correlação, o que em data science não costuma ser problema
			- Análise de discriminante funciona com preditores contínuos e categóricos e com resultados categóricos
			- Usando a matriz de covariância é calculado a função de discriminante linear que separa os registros em classes
		</section>
		<section data-markdown="">
			##	Regressão Logística
			- É análoga a múltiplas regressões lineares, mas o resultado é binário
			- É um procedimento estruturado, não sendo centrado em dados como KNN e Naive Bayes
			- É muito rápido e por isso sua popularidade
			- __Logit__ é a função que mapeia a probabilidade de pertencer a uma classe com alcance de 	&plusmn;&infin;
		</section>
		<section data-markdown="">
			##	Regressão Logística
			- A regressão logística funciona utilizando a logística de resposta e o _Logit_
			- Eles mapeia uma probabilidade de 0-1 para uma escale que seja mais adequada para a regressão linear
			- Vimos a regressão no formato
			\begin{aligned}
			p = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_qx_q 
			\end{aligned}
		</section>
		<section data-markdown="">
			##	Regressão Logística
			- Mas ajustar a esse modelo não garante que o resultado esteja entre 0 e 1, como toda probabilidade deve estruturado
			- Por isso usamos a função logística
			\begin{aligned}
			p = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_qx_q)}}
			\end{aligned}
			- Essa transformação garante que p esteja entre 0 e 1
			- Para tirar a expressão exponencial do denominador, devemos considerar chance ao invés de probabilidade
		</section>
		<section data-markdown="">
			##	Regressão Logística
			\begin{aligned}
			Odd(Y=1) = \frac{p}{1-p}
			\end{aligned}
			\begin{aligned}
			p = \frac{Odds}{1+Odds}
			\end{aligned}
			- Chance a probabilidade de um evento dividido pela probabilidade de que ele não ocorra
			- Se a probabilidade de um cavalo ganhar é de 0.5, a de não ganhar é 1 - 0.5 = 0.5
			- Então, as chances são de 1
		</section>
		<section data-markdown="">
			##	Regressão Logística
			\begin{aligned}
			Odds(Y=1) = e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_qx_q}
			\end{aligned}
			\begin{aligned}
			log(Odds(Y=1)) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_qx_q
			\end{aligned}
			- O logit mapeia a probabilidade p de (0,1) para (-&infin;,+&infin;)
			- Agora podemos classificar, temos um modelo linear para prever a probabilidade
			- Podemos usar um limite para mapear qualquer registro com probabilidade maior que ele a uma classe
		</section>
<section>
	<pre><code>
p &lt;- seq(from=0.01, to=.99, by=.01)
df &lt;- data.frame(p = p ,
	logit = log(p/(1-p)),
	odds = p/(1-p))
library(ggplot2)
ggplot(data=df, aes(x=p, y=logit)) +
	geom_line() +
	labs(x = 'p', y='logit(p)') +
	theme_bw()
			
	</code></pre>
</section>
		<section data-markdown="">
			![Função Logística](img/Modulo3FuncaoLogistica.png)
		</section>
		<section data-markdown="">
			## Regressão logística e GLM 
			- Apenas observamos o resultado binário, não o log da chance. Métodos estatísticos
			são usados para ajustar a curva
			- Regressão logística são um caso especial do modelo linear generalizado (GLM)
			- Foram criados para estender a regressão linear para outras configurações
			- Para usar a regressão logística em R, usamos a função glm com parâmetro `family='binomial'`
		</section>
		<section>
			<pre><code>
logistic_model &lt- glm(outcome ~ payment_inc_ratio + purpose_ + 
	home_ + emp_len_ + borrower_score,
	data=loan_data, family='binomial')

			</code></pre>
		</section>
		<section data-markdown="">
			## Modelos Lineares Generalizados
			- São a segunda mais importante classe de modelos, perdendo para regressão
			- Possuem dois componentes
				- Uma função de distribuição de probabilidade ou família (quando binomial)
				- Uma função que mapeia a resposta aos preditores (como a logit)
			- A logística é a mais comum
			- Outros exemplos são Poisson, binomial negativa e gamma. São usadas para modelar tempo decorrido
			- Tome cuidado ao trabalhar com essas outras 

		</section>
		<section data-markdown="">
			## Valores Preditos
			\begin{aligned}
			\hat{p} = \frac{1}{1 + e^{-\hat{Y}}}
			\end{aligned}
			`pred &lt;- predict(logistic_model)`
			 <br>
			`prob &lt;- 1/(1+exp(-pred))`
			- Poderíamos estabelecer o limite como 0.5. Na prática, um valor menor é necessário para casos raros
		</section>
		<section data-markdown="">
			### Coeficientes e Razão de Chances
			- Uma das vantagens da regressão logística é que nela é fácil adicionar novos dados
			- Para sua interpretação é importante entender o conceito de razão de chances
			\begin{aligned}
			razão de chances = \frac{Chance(Y = 1 | X = 1)}{Chance(Y=1 | X = 0)}
			\end{aligned}
			- Se a razão é 2, significa que a chance de Y=1 quando X=1 é duas vezes maioria
			- Na regressão logística &beta;1 é o log da razão de chances para Xp 
			- Para propósito de pequeno negócio o coeficiente foi 1.21, fazendo exp(1.21) a razão é de 3.37
		</section>

		<section data-markdown="">
			![Razão de Chances](img/Módulo3Razão.png)
			- Se o coeficiente cresce em 1, a razão de chances cresce em 2.72
		</section>
		<section data-markdown="">
			## Comparando com Linear
			- Ambos consideram uma função paramétrica entre o preditor e a resposta
			- A exploração dos coeficientes é semelhante
			- Também é possível aplicar spline e integração passo a passo
			- Diferenças
				- O ajustamento não usa menores quadrados, já que não são aplicados
				- A natureza e a análise de resíduos
		</section>
		<section data-markdown="">
			## Ajustando
			- Em Logistic o ajuste usa MLE _maximum likehood estimation_
			- O MLE encontra o modelo que melhor estima o log das chances para a resposta 1
			- Para fatores, devemos fazer como na regressão lineares
		</section>
		<section data-markdown="">
			## Avaliando o modelo
			- `summary(logistic_model)`
			- O p-value é mais relevante para identificar a importância da variável
			- Não temos RMSE nem R²
			- Alguns parâmetros do summary como dispersão podem ser ignorados, sendo mais relevantes para outros GLMs
		</section>
		<section>
			<pre><code>
logistic_gam &lt;- gam(outcome ~ s(payment_inc_ratio) + purpose_ + 
	home_ + emp_len_ + s(borrower_score),
	data=loan_data, family='binomial')
terms &lt;- predict(logistic_gam, type='terms')
partial_resid &lt;- resid(logistic_gam) + terms
df &lt;- data.frame(payment_inc_ratio = loan_data[, 'payment_inc_ratio'],
			   terms = terms[, 's(payment_inc_ratio)'],
			   partial_resid = partial_resid[, 's(payment_inc_ratio)'])

ggplot(df, aes(x=payment_inc_ratio, y=partial_resid, solid = FALSE)) +
geom_point(shape=46, alpha=.4) +
geom_line(aes(x=payment_inc_ratio, y=terms), 
		  color='red', alpha=.5, size=1.5) +
labs(y='Partial Residual') +
xlim(0, 25) +
theme_bw()
			</code>
			</pre>
		</section>
		<section data-markdown="">
			- Logística é parecida com Linear, mas o resultado é binário
			- Algumas transformações são necessárias
			- É popular por ser rápido e gerar um modelo que pode ser pontuado a novos dados sem recalcular
		</section>
		<section data-markdown="">
			## Avaliando modelos de Classificação
			- Uma forma simples é contar a proporção de predições corretas
			<br>
			\begin{aligned}
			acurácia = \frac{\sum{Verdadeiro Positivo} + \sum{Verdadeiro Negativo}}{Tamanho Amostra}
			\end{aligned}
		</section>
		<section data-markdown="">
			## Matriz de Confusão 
			- Mostra as predições corretas e incorretas categorizados pelo tipo da resposta
		</section>
		<section>
<pre><code>
pred &lt;- predict(logistic_gam, newdata=loan_data)
pred_y &lt;- as.numeric(pred > 0)
true_y &lt;- as.numeric(loan_data$outcome=='default')
true_pos &lt;- (true_y==1) & (pred_y==1)
true_neg &lt;- (true_y==0) & (pred_y==0)
false_pos &lt;- (true_y==0) & (pred_y==1)
false_neg &lt;- (true_y==1) & (pred_y==0)
conf_mat &lt;- matrix(c(sum(true_pos), sum(false_pos),
	sum(false_neg), sum(true_neg)), 2, 2)
colnames(conf_mat) &lt;- c('Yhat = 1', 'Yhat = 0')
rownames(conf_mat) &lt;- c('Y = 1', 'Y = 0')
conf_mat

# precision
conf_mat[1,1]/sum(conf_mat[,1])
# recall
conf_mat[1,1]/sum(conf_mat[1,])
# specificity
conf_mat[2,2]/sum(conf_mat[2,])
</code></pre>
		</section>
		<section data-markdown="">
			## Problema de classes raras
			- Muitas vezes existe um desequilíbrio com uma classe sendo mais prevalente que outras
			- A classe rara costuma ser a classe de interesse, geralmente o 1
			- Geralmente classificar errôneamente um 1 como 0, costuma ser pior que um 0 como 1
			- Se uma fraude for identificada errôneamente, é feita uma análise manual para verificar
			- Se o evento raro ocorre com 0.1% de chance, um modelo que sempre responde 0 tem precisão de 99.9%,
			__isso é bom!?__
		</section>
		<section data-markdown="">
			## Precision, Recall e Specificity
			\begin{aligned}
			precision = \frac{\sum{Verdadeiro Positivo}}{\sum{Verdadeiro Positivo} + \sum{Falso Positivo}}
			\end{aligned}
			<br>
			- Mede a acurácia de predizer um resultado positivo
			<br>
			\begin{aligned}
			recall = \frac{\sum{Verdadeiro Positivo}}{\sum{Verdadeiro Positivo} + \sum{Falso Negativo}}
			\end{aligned}
			<br>
			- Recall ou sensitividade (usado, também, em biomedicina) mostra a força do modelo de medir corretamente os resultados
			positivos
		
		</section>
		<section data-markdown="">
			## Precision, Recall e Specificity
			\begin{aligned}
			specificity = \frac{\sum{Verdadeiro Negativo}}{\sum{Verdadeiro Negativo} + \sum{Falso Positivo}}
			\end{aligned}
			- Probabilidade do modelo de predizer um resultado negativo
		</section>
		<section data-markdown="">
			## Curva ROC
			- Existe um _tradeoff_ entre especificidade e recall, Identificar mais 1s pode classificar errôneamente mais 0s e 1s
			- A métrica que captura o tradeoff é a curva de  _Receiver Operating Characteristics_
			- Para gerar a curva:
				1. Ordene os registros pela probabilidade de ser 1
				1. Compute a especificidade e recall cumulativo
		</section>
		<section>
<pre><code>

idx <- order(-pred)
recall <- cumsum(true_y[idx]==1)/sum(true_y==1)
specificity <- (sum(true_y==0) - cumsum(true_y[idx]==0))/sum(true_y==0)
roc_df <- data.frame(recall = recall, specificity = specificity)
ggplot(roc_df, aes(x=specificity, y=recall)) +
	geom_line(color='blue') + 
	scale_x_reverse(expand=c(0, 0)) +
	scale_y_continuous(expand=c(0, 0)) + 
	geom_line(data=data.frame(x=(0:100)/100), aes(x=x, y=1-x),
			linetype='dotted', color='red') +
	theme_bw()
</code></pre>
<p>A linha pontilhada mostra um classificador 'aleatório'</p>
<p>Outra curva é de Precisão-Recall, mas agora é ordenado do maior para o menor</p>
		</section>
		<section data-markdown="">
			## AUC
			- Apesar de gráficamente útil, o ROC sozinho não tem uma única medida de performance
			- O AUC é a área a baixo da curva ROC
			- Um valor de 1 indica um classificador perfeito
			- 0.5 indica um classificador ineficáz 
			- `sum(roc_df recall[-1] * diff(1-roc_df specificity))`
		</section>
		<section data-markdown="">
			## Lift
			- AUC é uma boa métrica pois verifica o tradeoff entre acurácia e os 1s
			- Não trata o problema de classes raras , onde temos que deixar o limite a baixo de 0.5, podendo de 0.4, 0.3 ou menor!
			- Como resultado classificamos mais 1s, refletindo a importância
			- Modificar o limite aumenta as chances de identificar 1s ao custo de classificar errado 0s e 1s, qual é o melhor limite?

		</section>
		<section data-markdown="">
			- Acurácia é apenas uma das formas de avaliar o modelo
			- Métricas como recall, specificity e precision focam em características mais específicas de performance
			- AUC é uma métrica para identifica a habilidade do modelo de distinguir 1s de 0s
			- lift mostra quão efetivo o modelo é em identificar 1s
		</section>
		<section data-markdown="">
			## Técnicas para dados desbalanceados
			- Vimos o caso de classe raras
			- Veremos outras técnicas para tratar esse mesmo tipo de problema
		</section>
		<section data-markdown="">
			## Undersampling
			- A ideia é que a classe dominante possui muitos registros redundantes
			- Quanto mais distinguível forem as duas classes, menos dados precisamos
			- Assim, removemos os registros que estão em 'excesso' antes de treinar o modelo
			- A classe comum com a variabilidade dos preditores faz com que mesmo preditores que indicariam o raro,
			possuem equivalentes da mais comum
		</section>
		<section>
			<pre><code>
mean(full_train_set$outcome=='default')
# 19% dos dados é default
full_model <- glm(outcome ~ payment_inc_ratio + purpose_ + 
	home_ + emp_len_+ dti + revol_bal + revol_util,
	data=full_train_set, family='binomial')
pred <- predict(full_model)
mean(pred > 0)
# Apenas 0.39% foram preditos como default, 1/12 do valor esperado!
			</code></pre>
		</section>
		<section data-markdown="">
			## Oversampling e Ajustes para Cima e Baixo
			- Uma crítica ao undersampling é que deixamos de usar dados, não usando todos os dados a nossa disposição
			- Para um data set pequeno, isso pode ser um problema!
			- Uma solução é, ao invés de downsampling o caso dominante, fazer oversample do caso raro
			- Podemos fazer isso usando reposição (bootstrap) nos casos raros
			- Para alcançar um efeito similar podemos ajustar os dados
		</section>
		<section>
			<pre><code>
wt <- ifelse(full_train_set$outcome=='default', 1/mean(full_train_set$outcome == 'default'), 1)
full_model <- glm(outcome ~ payment_inc_ratio + purpose_ + 
	home_ + emp_len_+ dti + revol_bal + revol_util,
	data=full_train_set, weight=wt, family='quasibinomial')
pred <- predict(full_model)
mean(pred > 0)
# A soma dos pesos para default e não default é quase igual
			</code></pre>
		</section>
		<section data-markdown="">
			## Geração de dados
			- É uma variação de upsampling, onde geramos perturbação nos dados existentes para gerar novos dados
			- O método SMOTE encontra um registro similar ao que está sendo upsampled e cria um registro sintético
			que é uma média aleatória do registro original e seus vizinhos
			- O pacote mais usado em R para geração de dados é o `unbalanced` que possui um algoritmo `Racing`
			para encontrar o melhor método
			- SMOTE também é implementado pelo pacote `FNN` e costuma ser uma boa escolha
		</section>
		<section data-markdown="">
			## Classificação Baseada em Custo
			- Muitas vezes é possível estimar custos relacionados com o resultado positivo ou negativo
			- Se R é o custo de um default e o retorno de paid-off é R
			\begin{aligned}
				retorno = P(Y=0) \times R + P(Y = 1) \times C
			\end{aligned}
			- Ao invés de identificar a probabilidade de não pagar, podemos verificar se há um retorno positivo
			- Nesse caso, a probabilidade de inadimplência é um passo intermediário
		</section>
		<section data-markdown="">
			## Explorando Predições
			- Em muitos casos, uma única métrica como AUC não consegue capturar todos aspectos relevantes para a situação
			- Na figura, os valores a cima das curvas são default
			![Comparação entre regras de classificação de métodos diferentes](img/Modulo3Metodos.png)
		<section data-markdown="">
			- Dados desbalanceados são problema para algoritmos de classificação
			- Uma estratégia é balancear os dados com undersample ou Oversampling
			- SMOTe permite criar dados sintéticos
			- Dados desbalanceados indica que classificar corretamente uma classe tem mais valor, essa razão deve ser levada em conta
		</section>
			</div>
		</div>

		<script src="../lib/js/head.min.js"></script>
		<script src="../js/reveal.js"></script>
		<script src="../js/base.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: 'c/t',

				transition: 'slide', // none/fade/slide/convex/concave/zoom
				math: {
					// mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					config: 'TeX-AMS_HTML-full',
					TeX: {
						Macros: {
							R: '\\mathbb{R}',
							set: [ '\\left\\{#1 \\; ; \\; #2\\right\\}', 2 ]
						}
					}
				},
				// Optional reveal.js plugins
				dependencies: [
          { src: '../lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: '../plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: '../plugin/zoom-js/zoom.js', async: true },
					{ src: '../plugin/notes/notes.js', async: true },
					{ src: '../plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
