<!doctype html>
<html lang="pt">

	<head>
		<meta charset="utf-8">

		<title>Estatística para Data Science II</title>

		<meta name="author" content="Fernando Antonio Marques Filho">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="../css/reveal.css">
		<link rel="stylesheet" href="../css/misc.css">
		<link rel="stylesheet" href="../css/theme/sky.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="../lib/css/zenburn.css">
		<style>
			img {
				border : 0px !important;
			}
		</style>
		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? '../css/print/pdf.css' : '../css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>
	<body>
		<div class="reveal">
			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h3>Regressão e Predição</h3>	
					<img src="./img/logopos.webp" alt="">
					<p>
						<small>Created by Fernando Marques </small>
					</p>
		</section>
		<section data-markdown="">
			## Valores categóricos na Regressão 
			- A regressão exige que os valores sejam inteiros
			- O que fazer com as variáveis categóricas?
				- Exemplo de categoria: motivo de empréstimo
			- A forma mais comum é utilizar as variáveis dummy
		</section>
		<section data-markdown="">
			## Variáveis dummy
			- `head(house[,"PropertyType"])`
			- `levels(house[,"PropertyType"])`
			- Para usar essas variáveis temos que converter elas para variáveis binárias
			- `prop_type_dummies &lt;- model.matrix(~PropertyType -1, data=house)`
			- `head(prop_type_dummies)`
			- Esse tipo de divisão é chamado de _one hot encoder_ ou codificador quente
		</section>
		<section data-markdown="">
			## Variáveis dummy
			- Para regressão uma variável com P valores é representada com P-1 colunas já que na regressão tem o termo de interceptação
			- Com esse termo, tendo P-1, sabemos o valor para P
			- Por padrão R usa o primeiro fator como interceptação
			- `lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + 
			Bedrooms +  BldgGrade + PropertyType, data=house)`
			- Single Family  vale 85000 a menos e Townhouse 150000 a menos se comparados a Multiplex
			- Existem outras formas de codificação como sum contrasts, polynomial e outras mas as mais usadas são reference e one hot encoder
		</section>
		<section data-markdown="">
			## Fatores com muitos leveis
			- Algumas variáveis podem gerar um grande número de dummies binários
			- No caso das casas, existem 82 zip codes
			- `table(house$ZipCode)`
			- Seriam 81 novos coeficientes, sendo que só tinhamos 5!
			- Alguns zip code tem apenas 1 venda, o que é problemático
			- Mas não podemos desconsiderar essa variável! _location location location_
			- Os primeiros 03 dígitos indicam regiam submetropolitana, mas 983 tem poucas vendas
		</section>
		<section>
<pre><code class="R">
library(dplyr)
zip_groups &lt- house %>%
mutate(resid = residuals(house_lm)) %>%
group_by(ZipCode) %>%
summarize(med_resid = median(resid),
			cnt = n()) %>%
# sort the zip codes by the median residual
arrange(med_resid) %>%
mutate(cum_cnt = cumsum(cnt),
		ZipGroup = factor(ntile(cum_cnt, 5)))
house &lt- house %>%
left_join(select(zip_groups, ZipCode, ZipGroup), by='ZipCode')
</code></pre>
		</section>
		<section data-markdown="">
			## Fatores com muitos leveis
			- O resíduo da média é calculado para cada zipcode e a função ntile separa os códigos em grupos de 5
			- O conceito de usar resíduos para ajudar no ajuste da regressão é fundamental no processo de modelagem
		</section>
		<section data-markdown="">
			## Categorias ordenadas
			- Algumas categorias podem ser ordenadas, assim, usar a condição de decodificação perderia informação
			- Uma nota pode ser boa, média, ruim ...
			- [Ordered Factors](https://www.dummies.com/programming/r/how-to-work-with-ordered-factors-in-r/)
		</section>
		<section data-markdown="">
			- Fatores podem ser convertidos em números para fazer a regressão
			- O método mais comum de codificar um fator é usando P-1 variáveis dummy
			- Um fator com muitos níveis pode ter que ser convertido
			- Alguns fatores são ordenados e podem ser representados por números
		</section>
		<section data-markdown="">
			> Em data science o uso mais importante de regressão é predizer alguma variável dependente. Ainda assim, podemos
			conhecer a equação e entender o relacionamento das variáveis preditoras e a previsão 
		</section>
		<section data-markdown="">
			## Preditores correlacionados
			- Nas regressões múltiplas os preditores costumam ser correlacionados
			- Se olharmos os coeficientes do step_lm veremos que o coeficiente de quarto é negativo! HOW!?!?!
			- Adicionar um quarto à casa reduz seu preço?????
			- Casas grandes tendem a ter mais quartos, e é o tamanho que eleva o preço da casa
			- Em duas casa de mesmo tamanho, a que tiver mais quartos terá quartos menores
			- Preditores correlacionados podem dificultar a interpretação do valor de cada coeficiente
			- As variáveis para quartos, tamanho da casa e número de banheiros estão correlacionadas
		</section>
		<section data-markdown="">
			## Preditores correlacionados
			- `update(step_lm, . ~ . -SqtToLiving - SqFtFinBasement - Bathrooms)`
			- A função update pode ser usada para adicionar ou remover variáveis de um modelo
			- Agora a variável Bedrooms é positiva, mas funciona como um proxy para SqtToLiving

		</section>
		<section data-markdown="">
			## Multicolinearidade
			- Um caso extremo de variáveis correlacionadas é a multicolinearidade , uma condição onde há redundância
			entre as variáveis preditoras
			- Multicolinearidade perfeita é quando uma variável preditora pode ser descrita como uma regressão linear das outras
			- Multicolinearidade pode acontecer quando:
				- Uma variável é incluída múltiplas vezes por erro
				- São usadas P dummies
				- Duas variáveis são quase perfeitamente correlacionadas
		</section>
		<section data-markdown="">
			## Multicolinearidade
			- Multicolinearidade deve ser tratada em regressões! Removendo as variáveis correlacionadas
			- Não é um problema em outros tipos de métodos como arvores, clustering ou vizinhos mais próximos
		</section>
		<section data-markdown="">
			## Variável de Confusão ou Confundimento
			- Com variáveis correlacionadas o problema é adicionar muitas variáveis com relação similar a previsão
			- No confundimento o problema é a omissão, alguma variável importante não foi adicionada na regressão
			- Se adicionarmos ZipGroup a regressão agora teremos Bathroom e SqFtLot positivos
			- O coeficiente do Zip também é alto, podendo adicionar até $340,000
			- [Explicação sobre Variável de Confusão](https://www.statisticshowto.datasciencecentral.com/experimental-design/confounding-variable/)
		</section>
		<section data-markdown="">
			## Interações e Efeito principal
			- Existe uma diferença entre efeitos principais ou variáveis independentes e a interação entre os efeitos
			- Uma suposição implícita quando fazemos regressão é que o preditor e a resposta são independentes de outros preditores
			- Nem sempre isso acontece...
		</section>
		<section data-markdown="">
			## Interações e Efeito principal
			- Vimos o efeito do ZipCode nos outros coeficientes
			- Sabemos que no setor imobiliário localização é tudo. Assim, uma casa grande em uma área nobre não terá o mesmo
			preço que uma casa na perifería 
			- Para incluir interação entre variáveis em R usamos o operador *
			- `lm(AdjSalePrice ~  SqFtTotLiving*ZipGroup + SqFtLot + Bathrooms + Bedrooms +	BldgGrade + PropertyType,  data=house, na.action=na.omit)`
		</section>
		<section data-markdown="">
			## Interações e Efeito principal
			- São criados relacionamentos entre SqFtTotLiving e Zip Group 
			- A diferença de aumentar um pé quadrado dos Zips mais baratos para o mais caro é de 2.9!
			- $118 vs $348
			- Pode ser difícil decidir quais termos devem ser incluídos no modelo para isso podemos
				- Conhecimento do negócio e intuição
				- Stepwise
				- Usar regressão penalizada [exemplos](http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/)
				- Usar modelos de árvores
		</section>
		<section data-markdown="">
			- Por causa da correlação entre preditores é importante tomar cuidado com regressões multivariadas
			- Multicolinearidade pode causar instabilidade na regressão
			- Um variável de confusão é um preditor importante que foi omitido do modelo e pode levar a relacionamentos errados
			- Um termo de interação entre duas variáveis é importante quando o relacionamento entre as variáveis é interdependente
		</section>
		<section data-markdown="">
			## Diagnóstico de Regressão 
			### Outliers, Parte II - _O Retorno_
			- Outliers são valores que se distanciam das outras observações
			- Assim como na localização e variabilidade, os outliers também devem ser tratados na regressão
			- Não existe uma teoria estatística para reparar outliers, existem algumas boas práticas como 1.5 IQR
			- Os resíduos padronizados são a forma de encontrar outliers na regressão
			- Resíduo padronizado pode ser interpretado como o números de erros padrão distante da regressão
		</section>
		<section >
<pre><code class="R">
house_98105 &lt;- house[house$ZipCode == 98105,]
lm_98105 &lt;- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + 
	Bedrooms + BldgGrade, data=house_98105)
sresid &lt;- rstandard(lm_98105)
idx &lt;- order(sresid, decreasing=FALSE)
sresid[idx[1]]
resid(lm_98105)[idx[1]]

house_98105[idx[1], c('AdjSalePrice', 'SqFtTotLiving', 'SqFtLot',
	'Bathrooms', 'Bedrooms', 'BldgGrade')]
</code></pre>
		</section>
		<section data-markdown="">
			## Outliers
			- Houve um erro na estimativa de $757,753!!!
			- A casa foi vendida por um preço muito menor do que valia
			- Outros casos para outliers são dados entrados de forma errada e erro de unidades
			- Para big data outliers podem não ser um problema 
			- Porém, outliers são centrais na detecção de anomalias como na detecção de fraudes
		</section>
		<section data-markdown="">
			## Valores influentes
			- Um valor cuja a ausência muda a equação de regressão
			- Consideramos que o ponto tem uma alta influência na regressão
			- Esse valor não é necessáriamente um outliers
			- Além do resíduo padronizado também existe o _hat-value_ e a distância de Cook
			\begin{aligned}
			Valor Hat &lt; 2(P+1)/n 
			\end{aligned}
			\begin{aligned}
			Valor Cook &lt; \frac{4}{n-P-1}
			\end{aligned}
		</section>
		<section data-markdown="">
			![Grafico Valor Influente](img/Modulo1ValorInfluente.png)
		</section>
		<section>
<pre><code>
seed &lt;- 11
set.seed(seed)
x &lt;- rnorm(25)
y &lt;- -x/5 + rnorm(25)
x[1] &lt;- 8
y[1] &lt;- 8
plot(x, y, xlab='', ylab='', pch=16)
model &lt;- lm(y~x)
abline(a=model$coefficients[1], b=model$coefficients[2], col="blue", lwd=3)
model &lt;- lm(y[-1]~x[-1])
abline(a=model$coefficients[1], b=model$coefficients[2], col="red", lwd=3, lty=2)
		
</code></pre>
		</section>
		<section>
<pre><code>
std_resid &lt;- rstandard(lm_98105)
cooks_D &lt;- cooks.distance(lm_98105)
hat_values &lt;- hatvalues(lm_98105)
df &lt;- data.frame(hat_value=hat_values, std_resid = std_resid, cooks_D = cooks_D)
ggplot(data=df, aes(x=hat_value,y=std_resid,size=cooks_D)) + 
   geom_point(shape=1) + geom_hline(yintercept = 2.5, linetype = "dashed") + 
   geom_hline(yintercept = -2.5, linetype = "dashed")
</code></pre>
		</section>
		<section>
<pre><code class="R">
lm_98105_inf &lt- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + 
	Bathrooms +  Bedrooms + BldgGrade,
	subset=cooks_D&lt.08, data=house_98105)

df &lt- data.frame(lm_98105$coefficients,
	lm_98105_inf$coefficients)
names(df) &lt- c('Original', 'Influential Removed')
ascii((df),
include.rownames=TRUE, include.colnames=TRUE, header=TRUE,
digits=rep(0, 3), align=c("l", "r", "r") ,
caption="Comparison of regression coefficients with the full data and with influential data removed")
</code></pre>
		</section>
		<section data-markdown="">
			## Valores influentes
			- Notem que os valores de Bathrooms mudam radicalmente!
			- Para dados com muitas observações é pouco provável que uma única observação tenha peso suficiente para
			influenciar a regressão
			- Para propósitos de encontrar anomalias, valores influentes são bastante úteis
		</section>
		<section data-markdown="">
			## Heterocedasticidade
			- Para a maioria dos casos o estimador de quadrados mínimos é uma das melhores opções
			- Inferências normais assumem que os resíduos são normalmente distribuídos, tem a mesma variância e são
			independentes
			- Isso é particularmente importante quando calculamos o intervalo de confiança para os valores previstos, já que é baseado nos resíduos
			- Heterodasticidade é a falta de uma variação no resíduo ao longo dos valores preditos
			- Indica que erros de predição são diferentes para amplitudes diferentes
		</section>
		<section>
			<pre><code class="R">
df &lt;- data.frame(
	resid = residuals(lm_98105),
	pred = predict(lm_98105))
ggplot(df, aes(pred, abs(resid))) +
	geom_point() +
	geom_smooth() 

ggplot() + geom_qq_line(aes(sample=std_resid)) + geom_qq(aes(sample=std_resid))
			</code></pre>
		</section>
		<section data-markdown="">
			## Resíduos parciais e não linearidade
			- Gráficos de resíduo parcial são uma forma de explicar a relação entre o preditor e o resultado
			- Também ajuda a detectar outliers
			- A ideia é isolar o relacionamento entre a variável preditora e a resposta, considerando todos os outros preditores
			- O resíduo parcial combina o valor previsto por uma variável e o resíduo da regressão como um todo
			\begin{aligned}
			Partial residual = residual + \hat{b}_iX_i
			\end{aligned}
		</section>
		<section>
<pre><code>
terms &lt;- predict(lm_98105, type='terms')
partial_resid &lt;- resid(lm_98105) + terms

df &lt;- data.frame(SqFtTotLiving = house_98105[, 'SqFtTotLiving'],
	Terms = terms[, 'SqFtTotLiving'],
	PartialResid = partial_resid[, 'SqFtTotLiving'])
ggplot(df, aes(SqFtTotLiving, PartialResid)) +
	geom_point(shape=1) + scale_shape(solid = FALSE) +
	geom_smooth(linetype=2) + 
	geom_line(aes(SqFtTotLiving, Terms))  
</code></pre>
		</section>
		<section data-markdown="">
			## Resíduos parciais e não linearidade
			- A regressão subestima o valor das casas a baixo de 1000 pés quadrados e superestima o preço das casas
			entre 2000 e 3000, existem poucos pontos acima de 4000 para tirar conclusão
			- A diferença faz sentido, adicionar 500ft² a uma casa pequena faz mais diferença que adicionar os mesmos
			500 ft² a uma casa grande
			- Esse tipo de comportamento sugere que o termo SqFtToLiving não seja linear!
		</section>
		<section data-markdown="">
			## Polinomial
			\begin{aligned}
			Y = b_0 + b_1X +b_2X^2 + e
			\end{aligned}
			- `lm(AdjSalePrice ~ poly(SqFtTotLiving, 2) + SqFtLot +	BldgGrade + Bathrooms +  Bedrooms, 	data=house_98105) `
		</section>
		<section>
	<pre><code>
lm_poly &lt;- lm(AdjSalePrice ~  poly(SqFtTotLiving, 2) + SqFtLot + 
	BldgGrade +  Bathrooms +  Bedrooms,
	data=house_98105)
terms &lt;- predict(lm_poly, type='terms')
partial_resid &lt;- resid(lm_poly) + terms

df &lt;- data.frame(SqFtTotLiving = house_98105[, 'SqFtTotLiving'],
			 Terms = terms[, 1],
			 PartialResid = partial_resid[, 1])
ggplot(df, aes(SqFtTotLiving, PartialResid)) +
	geom_point(shape=1) + scale_shape(solid = FALSE) +
	geom_smooth(linetype=2) + 
	geom_line(aes(SqFtTotLiving, Terms))+
	theme_bw()
	</code></pre>
		</section>
		<section data-markdown="">
			## Spline?
			- As regressões polinomiais só conseguem atingir uma certa quantidade de curvatura
			- Simplesmente aumentar a ordem do polinômio não é a melhor forma
			- O Spline permite interpolar entre dois valores fixos
			- A definição técnica de spline é uma série de polinômios contínuos
			- Os polinômios são conectados a uma série de pontos fixos chamados de nós
			- Contrário das outras regressões, os coeficientes são de difícil entendimento, sendo melhor ver o gráfico
		</section>
		<section data-markdown="">
			![Exemplo Spline Real](img\Modulo1Spline.jpg)
		</section>
		<section >
<pre><code>
library(splines)
knots &lt;- quantile(house_98105$SqFtTotLiving, p=c(.25, .5, .75))
lm_spline &lt;- lm(AdjSalePrice ~ bs(SqFtTotLiving, knots=knots, degree=3) +  SqFtLot +  
	Bathrooms + Bedrooms + BldgGrade,  data=house_98105)
terms1 &lt;- predict(lm_spline, type='terms')
partial_resid1 &lt;- resid(lm_spline) + terms

df1 &lt;- data.frame(SqFtTotLiving = house_98105[, 'SqFtTotLiving'],
	Terms = terms1[, 1],
	PartialResid = partial_resid1[, 1])
ggplot(df1, aes(SqFtTotLiving, PartialResid)) +
	geom_point(shape=1) + scale_shape(solid = FALSE) +
	geom_smooth(linetype=2) + 
	geom_line(aes(SqFtTotLiving, Terms))+
	theme_bw()
</code></pre>				
		</section>
		<section data-markdown="">
			## Modelo Aditivo Generalizado
			- Do Ingles _Generalized Additive Models_
			- Suponha que você suspeite que a relação entre a resposta de uma variável preditora é não linear
			- Polinômios podem não ser flexíveis o suficiente e splines requerem que você especifique os nós
			- GAM é uma técnica para automaticamente ajustar um spline a regressão
			- A biblioteca mgcv permite fazer isso usando o s
		</section>
		<section>
<pre><code>
lm_gam &lt;- gam(AdjSalePrice ~ s(SqFtTotLiving) + SqFtLot +  Bathrooms +  Bedrooms + BldgGrade, 
              data=house_98105)
terms &lt;- predict.gam(lm_gam, type='terms')
partial_resid &lt;- resid(lm_gam) + terms

df &lt;- data.frame(SqFtTotLiving = house_98105[, 'SqFtTotLiving'],
	Terms = terms[, 5], PartialResid = partial_resid[, 5])
ggplot(df, aes(SqFtTotLiving, PartialResid)) +
  geom_point(shape=1) + scale_shape(solid = FALSE) +
  geom_smooth(linetype=2) + 
  geom_line(aes(SqFtTotLiving, Terms))  +
  theme_bw()
</code></pre>
		</section>
		<section data-markdown="">
			- Em regressão outliers são valores com resíduo alto
			- Multicolinearidade pode causar instabilidade na equação de regressão
			- Uma variável de confusão é um preditor que não foi levado em consideração
			- Um termo de interação é necessário de uma variável depende do nível de outra
			- Polinômios podem acomodar relacionamentos não lineares
			- Splines são séries de polinômios juntados no nó
			- GAM automatiza o processo de escolher os nós do spline
		</section>
			</div>
		</div>

		<script src="../lib/js/head.min.js"></script>
		<script src="../js/reveal.js"></script>
		<script src="../js/base.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: 'c/t',

				transition: 'slide', // none/fade/slide/convex/concave/zoom
				math: {
					// mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					config: 'TeX-AMS_HTML-full',
					TeX: {
						Macros: {
							R: '\\mathbb{R}',
							set: [ '\\left\\{#1 \\; ; \\; #2\\right\\}', 2 ]
						}
					}
				},
				// Optional reveal.js plugins
				dependencies: [
          { src: '../lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: '../plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: '../plugin/zoom-js/zoom.js', async: true },
					{ src: '../plugin/notes/notes.js', async: true },
					{ src: '../plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
